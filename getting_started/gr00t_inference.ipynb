{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1b9963",
   "metadata": {},
   "source": [
    "# gr00t n 1.5 deployment details\n",
    "### they use GR1 as example, so we do not know much details about Droid.\n",
    "### also we do not know how thet train the model on droid\n",
    "- image size(1,256,256,3) | uint8| padding and resize, like pi0\n",
    "- what external_1 nad 2 means? which is left or right? For droid, ext1 is left, ext2 is right\n",
    "- they have already binarized gripper action in their code? action=1 means close, action=0 means open\n",
    "- rotation is 'rpy' euler angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01026d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gr00t\n",
    "\n",
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "from gr00t.model.policy import Gr00tPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the following paths\n",
    "MODEL_PATH = \"nvidia/GR00T-N1.5-3B\"\n",
    "\n",
    "# REPO_PATH is the path of the pip install gr00t repo and one level up\n",
    "REPO_PATH = os.path.dirname(os.path.dirname(gr00t.__file__))\n",
    "DATASET_PATH = os.path.join(REPO_PATH, \"demo_data/robot_sim.PickNPlace\")\n",
    "EMBODIMENT_TAG = \"oxe_droid\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0751d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
    "\n",
    "# can add [optional] denoising step in policy\n",
    "\n",
    "data_config = DATA_CONFIG_MAP[\"oxe_droid\"]\n",
    "modality_config = data_config.modality_config()\n",
    "modality_transform = data_config.transform()\n",
    "\n",
    "policy = Gr00tPolicy(\n",
    "    model_path=MODEL_PATH,\n",
    "    embodiment_tag=EMBODIMENT_TAG,\n",
    "    modality_config=modality_config,\n",
    "    modality_transform=modality_transform,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# print out the policy model architecture\n",
    "print(policy.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f540c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "modality_config = policy.modality_config\n",
    "\n",
    "print(modality_config.keys())\n",
    "\n",
    "for key, value in modality_config.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(key, value.shape)\n",
    "    else:\n",
    "        print(key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab9d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def resize_with_pad(images: np.ndarray, height: int, width: int, method=Image.BILINEAR) -> np.ndarray:\n",
    "    \"\"\"Replicates tf.image.resize_with_pad for multiple images using PIL. Resizes a batch of images to a target height.\n",
    "\n",
    "    Args:\n",
    "        images: A batch of images in [..., height, width, channel] format.\n",
    "        height: The target height of the image.\n",
    "        width: The target width of the image.\n",
    "        method: The interpolation method to use. Default is bilinear.\n",
    "\n",
    "    Returns:\n",
    "        The resized images in [..., height, width, channel].\n",
    "    \"\"\"\n",
    "    # If the images are already the correct size, return them as is.\n",
    "    if images.shape[-3:-1] == (height, width):\n",
    "        return images\n",
    "\n",
    "    original_shape = images.shape\n",
    "\n",
    "    images = images.reshape(-1, *original_shape[-3:])\n",
    "    resized = np.stack([_resize_with_pad_pil(Image.fromarray(im), height, width, method=method) for im in images])\n",
    "    return resized.reshape(*original_shape[:-3], *resized.shape[-3:])\n",
    "\n",
    "\n",
    "def _resize_with_pad_pil(image: Image.Image, height: int, width: int, method: int) -> Image.Image:\n",
    "    \"\"\"Replicates tf.image.resize_with_pad for one image using PIL. Resizes an image to a target height and\n",
    "    width without distortion by padding with zeros.\n",
    "\n",
    "    Unlike the jax version, note that PIL uses [width, height, channel] ordering instead of [batch, h, w, c].\n",
    "    \"\"\"\n",
    "    cur_width, cur_height = image.size\n",
    "    if cur_width == width and cur_height == height:\n",
    "        return image  # No need to resize if the image is already the correct size.\n",
    "\n",
    "    ratio = max(cur_width / width, cur_height / height)\n",
    "    resized_height = int(cur_height / ratio)\n",
    "    resized_width = int(cur_width / ratio)\n",
    "    resized_image = image.resize((resized_width, resized_height), resample=method)\n",
    "\n",
    "    zero_image = Image.new(resized_image.mode, (width, height), 0)\n",
    "    pad_height = max(0, int((height - resized_height) / 2))\n",
    "    pad_width = max(0, int((width - resized_width) / 2))\n",
    "    zero_image.paste(resized_image, (pad_width, pad_height))\n",
    "    assert zero_image.size == (width, height)\n",
    "    return zero_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class RealSenseCamera:\n",
    "    def __init__(self, serial_number, width=640, height=480, fps=30):\n",
    "        self.serial = serial_number\n",
    "        self.pipeline = rs.pipeline()\n",
    "        self.config = rs.config()\n",
    "        self.config.enable_device(self.serial)\n",
    "        self.config.enable_stream(rs.stream.color, width, height, rs.format.bgr8, fps)\n",
    "        self.pipeline.start(self.config)\n",
    "\n",
    "\n",
    "    def get_image(self):\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not color_frame:\n",
    "            return None\n",
    "\n",
    "        bgr = np.asanyarray(color_frame.get_data())\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return rgb\n",
    "\n",
    "    def release(self):\n",
    "        self.pipeline.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# camera serial numbers\n",
    "serial_left = \"948522071060\"\n",
    "serial_wrist = \"815412071252\"\n",
    "serial_right = \"838212074411\"\n",
    "\n",
    "# initialize cameras\n",
    "cam_left = RealSenseCamera(serial_left)\n",
    "cam_wrist = RealSenseCamera(serial_wrist)\n",
    "cam_right = RealSenseCamera(serial_right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b89353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_left = cam_left.get_image()\n",
    "img_wrist = cam_wrist.get_image()\n",
    "img_right = cam_right.get_image()\n",
    "\n",
    "# img_left = cv2.resize(img_left, (256, 256))  # shape (256, 256, 3)\n",
    "# img_wrist = cv2.resize(img_wrist, (256, 256))  # shape (256, 256, 3)\n",
    "# img_right = cv2.resize(img_right, (256, 256))  # shape (256, 256, 3)\n",
    "\n",
    "img_left = resize_with_pad(cam_left.get_image(), 256, 256)\n",
    "img_wrist = resize_with_pad(cam_wrist.get_image(), 256, 256)\n",
    "img_right = resize_with_pad(cam_right.get_image(), 256, 256)\n",
    "\n",
    "# combine the two images\n",
    "combined = np.hstack((img_left, img_wrist, img_right))  # shape (256, 768, 3)\n",
    "\n",
    "# display the combined image\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(combined)\n",
    "plt.title(\"Left + Wrist + Right \")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c967dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_left.release()\n",
    "cam_wrist.release()\n",
    "cam_right.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "step_data = {\n",
    "    'video.exterior_image_1': np.random.randint(0, 255, (1, 256, 256, 3), dtype=np.uint8),\n",
    "    'video.exterior_image_2': np.random.randint(0, 255, (1, 256, 256, 3), dtype=np.uint8),\n",
    "    'video.wrist_image': np.random.randint(0, 255, (1, 256, 256, 3), dtype=np.uint8),\n",
    "    'state.eef_position': np.random.randn(1, 3).astype(np.float32),\n",
    "    'state.eef_rotation': np.random.randn(1, 3).astype(np.float32),\n",
    "    'state.gripper_position': np.random.randn(1, 1).astype(np.float32),\n",
    "    'annotation.language.language_instruction': ['pick up the red block and place it on the green platform']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_data['state.eef_position'].shape\n",
    "step_data['state.eef_rotation'].shape\n",
    "step_data['state.gripper_position'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f15757",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_action = policy.get_action(step_data)\n",
    "for key, value in predicted_action.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vla_policy_client\n",
    "import Pyro5.api\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "ns = Pyro5.api.locate_ns()  # Locate the name server\n",
    "uri = ns.lookup(\"gr00t_controller\")  # Look up the registered object by name\n",
    "# uri = \"PYRO:obj_117a8599f4bc45e8a4ab4eb415348147@localhost:43953\"  # <-- change URI\n",
    "controller = Pyro5.api.Proxy(uri)\n",
    "\n",
    "\n",
    "video_buffer = []\n",
    "\n",
    "# prompt\n",
    "prompt = [\"pick the orange toy\"]\n",
    "\n",
    "# dummy action\n",
    "action = np.zeros((16,7), dtype=np.float32)\n",
    "action_list = action.tolist()  # convert to list for sending\n",
    "\n",
    "\n",
    "for step in range(5000):\n",
    "\n",
    "    print(f\"\\n=== Step {step} ===\")\n",
    "    data_to_send = {\n",
    "        \"action\": action_list,\n",
    "        \"step\": step\n",
    "    }\n",
    "    obs = controller.step(data_to_send)  # result is dict\n",
    "\n",
    "    img_left = resize_with_pad(cam_left.get_image(), 256, 256)\n",
    "    img_wrist = resize_with_pad(cam_wrist.get_image(), 256, 256)\n",
    "    img_right = resize_with_pad(cam_right.get_image(), 256, 256)\n",
    "\n",
    "    img_left = img_left[None, ...]     # shape: (1, 256, 256, 3)\n",
    "    img_wrist = img_wrist[None, ...]\n",
    "    img_right = img_right[None, ...]\n",
    "\n",
    "\n",
    "    # save images to video buffer\n",
    "    combined = np.hstack([img_left[0], img_right[0], img_wrist[0]])\n",
    "    video_buffer.append(combined)\n",
    "\n",
    "\n",
    "    step_data = {\n",
    "        'video.exterior_image_1': img_left,\n",
    "        'video.exterior_image_2': img_right,\n",
    "        'video.wrist_image': img_wrist,\n",
    "        'state.eef_position': np.array(obs['robot_pos'], dtype=np.float32),  # (1, 3)\n",
    "        'state.eef_rotation': np.array(obs['robot_rot'], dtype=np.float32),  # (1, 3)\n",
    "        'state.gripper_position': np.array(obs['gripper_state'], dtype=np.float32),  # (1, 1)\n",
    "        'annotation.language.language_instruction': prompt\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    predicted_action = policy.get_action(step_data)\n",
    "    pos = predicted_action['action.eef_position_delta']          # (16, 3)\n",
    "    rot = predicted_action['action.eef_rotation_delta']          # (16, 3)\n",
    "    grip = predicted_action['action.gripper_position']           # (16,)\n",
    "    if grip.ndim == 1:\n",
    "        grip = grip[:, np.newaxis]\n",
    "\n",
    "    action_concat = np.concatenate([pos, rot, grip], axis=-1)\n",
    "    print(\"Concatenated action:\", action_concat.shape)\n",
    "    action_list = action_concat.tolist()  # convert to list for sending\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import cv2\n",
    "\n",
    "\n",
    "gif_path = \"gr00t_deploy_2.gif\"\n",
    "imageio.mimsave(gif_path, video_buffer, duration=0.5)\n",
    "print(f\"GIF saved:{gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41386f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "IPyImage(filename=\"gr00t_deploy_2.gif\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gr00t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
